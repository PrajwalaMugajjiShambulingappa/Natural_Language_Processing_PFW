# -*- coding: utf-8 -*-
"""Lab_Assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JFEbWf9zQL_613Axu5p1zuKSvFiBqdDq
"""

#import pandas as pd

#from google.colab import files
#dataset = files.upload()
  
dataset = pd.read_csv("train.csv")

import re
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import CategoricalNB 
import pickle
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD
import csv

def normalize_text(dataset):
    processed_texts = []
    target_labels = []
    for index, row in dataset.iterrows():
        data = row['comment_text']
        target = row['toxic']
        if isinstance(data, str):
            for f in re.findall("([A-Z]+)", data):
                data = data.replace(f, f.lower())
            processed_text = re.sub(r"[^\w\s]", "", data)
            processed_text = re.split("\W", processed_text)
            processed_text = [i for i in processed_text if i != '']
            processed_texts.append(' '.join(processed_text))
            target_labels.append(target)
    return processed_texts, target_labels

def train_NB_model(path_to_train_file):
  dataset = pd.read_csv(path_to_train_file, nrows=10000)
  train_toxic_label = dataset["toxic"].tolist()
  dataset = dataset.iloc[:, 1:-5]
  processed_texts, target_labels = normalize_text(dataset)

  vectorizer = CountVectorizer()
  out = vectorizer.fit_transform(processed_texts)

  svd = TruncatedSVD(n_components=100)
  out = svd.fit_transform(out)

  train_model = GaussianNB()
  train_model.fit(out, train_toxic_label)

  with open("model_GaussianNB", "wb") as f:
    pickle.dump(train_model, f)

train_NB_model("/content/train.csv")

def train_NB_model(path_to_train_file):
  alpha=0.1
  dataset = pd.read_csv(path_to_train_file, nrows=10000)
  train_toxic_label = dataset["toxic"].tolist()
  dataset = dataset.iloc[:, 1:-5]
  processed_texts, target_labels = normalize_text(dataset)

  vectorizer = CountVectorizer()
  out = vectorizer.fit_transform(processed_texts)

  svd = TruncatedSVD(n_components=100)
  out = svd.fit_transform(out)

  train_model = GaussianNB(priors=[1-alpha, alpha])
  train_model.fit(out, train_toxic_label)

  with open("model_GaussianNB_reg", "wb") as f:
    pickle.dump(train_model, f)

train_NB_model("/content/train.csv")

def train_NB_model(path_to_train_file):
  dataset = pd.read_csv(path_to_train_file, nrows=10000)
  train_toxic_label = dataset["toxic"].tolist()
  dataset = dataset.iloc[:, 1:-5]
  
  dataset['text_len'] = dataset['comment_text'].apply(len)
  
  processed_texts, target_labels = normalize_text(dataset)

  vectorizer = CountVectorizer()
  out = vectorizer.fit_transform(processed_texts)

  svd = TruncatedSVD(n_components=100)
  out = svd.fit_transform(out)

  train_model = GaussianNB()
  train_model.fit(out, train_toxic_label)

  with open("model_GaussianNB_reg2", "wb") as f:
    pickle.dump(train_model, f)

train_NB_model("/content/train.csv")

def train_NB_model(path_to_train_file):
  from sklearn.model_selection import GridSearchCV

  dataset = pd.read_csv(path_to_train_file, nrows=10000)
  train_toxic_label = dataset["toxic"].tolist()
  dataset = dataset.iloc[:, 1:-5]
  processed_texts, target_labels = normalize_text(dataset)

  vectorizer = CountVectorizer()
  out = vectorizer.fit_transform(processed_texts)

  svd = TruncatedSVD(n_components=100)
  out = svd.fit_transform(out)

  params = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}
  grid_search = GridSearchCV(GaussianNB(), params, cv=5)

  grid_search.fit(out, train_toxic_label)

  with open("model_grid_search", "wb") as f:
    pickle.dump(grid_search, f)

train_NB_model("/content/train.csv")

def train_NB_model(path_to_train_file):
  from sklearn.model_selection import GridSearchCV

  dataset = pd.read_csv(path_to_train_file, nrows=10000)
  train_toxic_label = dataset["toxic"].tolist()
  dataset = dataset.iloc[:, 1:-5]
  processed_texts, target_labels = normalize_text(dataset)

  vectorizer = CountVectorizer()
  out = vectorizer.fit_transform(processed_texts)

  svd = TruncatedSVD(n_components=100)
  out = svd.fit_transform(out)

  train_model = BernoulliNB()
  train_model.fit(out, train_toxic_label)
  

  with open("model_BernoulliNB", "wb") as f:
    pickle.dump(train_model, f)

train_NB_model("/content/train.csv")

def train_NB_model(path_to_train_file):
  from sklearn.model_selection import GridSearchCV

  dataset = pd.read_csv(path_to_train_file, nrows=10000)
  train_toxic_label = dataset["toxic"].tolist()
  dataset = dataset.iloc[:, 1:-5]
  processed_texts, target_labels = normalize_text(dataset)

  vectorizer = CountVectorizer()
  out = vectorizer.fit_transform(processed_texts)

  svd = TruncatedSVD(n_components=100)
  out = svd.fit_transform(out)
  
  out = out + abs(out.min()) + 0.01

  train_model = MultinomialNB()
  train_model.fit(out, train_toxic_label)

  with open("model_MultinomialNB", "wb") as f:
    pickle.dump(train_model, f)

train_NB_model("/content/train.csv")

#from google.colab import files
#dataset = files.upload()
import pandas as pd
test_dataset_ALL = pd.read_csv("test.csv")

#from google.colab import files
#dataset = files.upload()
import pandas as pd
test_dataset_label = pd.read_csv("test_labels.csv")

#print(test_dataset_ALL)
def test_normalize_text(text_list):
    out = []
    for data in text_list:
        for f in re.findall("([A-Z]+)", data):
            data = data.replace(f, f.lower())
        data = data.replace('\n', '')
        data = data.replace('\t', '')
        out.append(data)
    return out

def test_NB_model(path_to_test_file, NB_model, path_to_test_label):
  
    with open(NB_model , "rb") as f:
        model = pickle.load(f)

    batch_size = 1000
    chunks = pd.read_csv(path_to_test_file, chunksize=batch_size)
    predictions_df = pd.DataFrame(columns=['id,' 'comment_text', 'Probability', 'Prediction'])

    for chunk in chunks:
        #chunk = chunk.drop(chunk.columns[0], axis=1)
        chunk['comment_text'] = test_normalize_text(chunk['comment_text'].tolist())

        vectorizer = CountVectorizer()
        chunk_df = vectorizer.fit_transform(chunk['comment_text'])

        svd = TruncatedSVD(n_components=100)
        chunk_df = svd.fit_transform(chunk_df)

        probabilty = model.predict_proba(chunk_df)
        predict = model.predict(chunk_df)

        chunk['Probability'] = probabilty[:,1]
        chunk['Prediction'] = predict

        predictions_df = pd.concat([predictions_df, chunk], ignore_index=True)

    output_file = path_to_test_file.split('.')[0] + '_output.csv'
    predictions_df.to_csv(output_file, index=False)

    #print(output_file)

    test_labels_df = pd.read_csv(path_to_test_label)
    test_labels_df = test_labels_df.iloc[:, :-5]
    
    predictions_df = pd.read_csv(output_file)
    predictions_df = predictions_df[['id', 'Probability', 'Prediction']]
    merged_df = pd.merge(predictions_df, test_labels_df, on='id')
    
    correct_predictions = 0
    total_predictions = 0
    
    for index, row in merged_df.iterrows():
      total_predictions += 1
      true_label = row['toxic']
      predicted_label = row['Prediction']
      if true_label == -1:
        true_label = 0
        if predicted_label == -1:
            predicted_label = 0
      if true_label == predicted_label:
        correct_predictions += 1

    accuracy = correct_predictions / total_predictions
    print("Accuracy: {:.2f}%".format(accuracy * 100))

#test_NB_model("/content/test.csv", "/content/model_GaussianNB", "/content/test_labels.csv")
#test_NB_model("/content/test.csv", "/content/model_GaussianNB_reg", "/content/test_labels.csv")
#test_NB_model("/content/test.csv", "/content/model_GaussianNB_reg2", "/content/test_labels.csv")
#test_NB_model("/content/test.csv", "/content/model_grid_search", "/content/test_labels.csv")
#test_NB_model("/content/test.csv", "/content/model_BernoulliNB", "/content/test_labels.csv")
#test_NB_model("/content/test.csv", "/content/model_MultinomialNB", "/content/test_labels.csv")